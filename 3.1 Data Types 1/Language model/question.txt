In natural language processing, language models are a frequently used tool. In tasks such as translation, question answering, text generation, etc., we often need to judge whether the sentences generated by the computer are close to the natural language that humans can speak, so we need to have a way of "evaluating how much a sentence is spoken by a real person" words" method, and this method is the language model.

Steps to change a language model:

Collect vast amounts of text from newspapers, journal articles, classics, and even internet forums. The collected text will be many sentences.

The sentences in the text are manually segmented, and the probability of occurrence of each segmented word is counted. The method of calculating the probability of each word in this way is called 1-gram.

Sentences tend to be structures made up of multiple words. So the probability of each word appearing should be related to the words that appear before the word. For example, we will say "晚餐真好吃"(dinner is delicious) but it is less likely to say "天氣真好吃"(the weather is delicious) because "delicious(真好吃)" is unlikely to be used Describe "weather(天氣)". Therefore, we can modify the 1-gram method and regard the probability of a word m as the conditional probability of P (m | one word before m, two words before m ... n-1 words before m). The method of calculating the probability of each word in this way is called n-gram.

Because the text is not too large (compared to the real world), if the n of the n-gram method is very large, the probability of each word appearing may be very low, because text collection needs to cover articles in various fields, and it is almost impossible to have Repeated sentences, so in general, n is usually less than 4.

When we are curious about the probability of a sentence appearing in the text, we can use the conditional probability that the components of the sentence appear in order to define the probability of the sentence. Taking the sentence "商品和服務)"(goods and services) as an example, we can use the following 2-gram model to calculate the probability of occurrence of this sentence


BOS stands for Begin of Sentence. EOS is the End of Sentence. P(商品 | BOS) is the probability that the word "商品" appears at the beginning of a sentence; P(和|商品) is the probability that the word "和" appears after the word "商品".

As long as one part of the function is completed in this exercise, we can input a word that accepts 5 numbers as a probability of a group of words 2-gram, which can be written as P (the first word | BOS), the second word For P(2nd word|1st word) and so on. And output this set of probabilities, in order to indicate the word, please output to 15 decimal places.


Input
5 consecutive decimals, representing the consecutive probability of a group of words. Each decimal is separated by a space.

ex : 0.5, 0.5, 0.5, 0.5, 0.5


Output
The probability of this group of words.

ex : 0.031250000000000